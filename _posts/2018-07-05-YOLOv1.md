---
layout:     post
title:      "目标检测文献阅读与理解之YOLOv1"
subtitle:   ""
date:       2018-07-05 22:00:00
author:     "Hebi"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - 深度学习
    - 图像识别
    - 目标检测
---

## detection 

1. each image被分成 S × S grid cells

2. each cell预测 B 个 `predicted bbox`（即 `(x, y, w, h, confidence)` ) ， C 个 `conditional class probabilities`

- 注1. 把each image 分成 S × S grid cells， 是指 each image 通过 CNNs 与 FCs 之后
会得到 一个 `S × S × （B × 5 + C）` 张量，即 each grid cell 都有 B 个 `predicted bbox`（即 `(x, y, w, h, confidence)` ) ， C 个 `conditional class probabilities`.

- 注2. predicted bbox 的 `confidence` 被定义为

$$
\text{confidence}
\triangleq \text{Pr(object)} * \text{IoU}_\text{pred}^\text{truth},
$$

其中

$$
\text{Pr(object)}=\bigg\{\begin{aligned}
1&,\quad \text{ if the center of object is in this cell;}\\
0&, \quad \text{othervise}.
\end{aligned}
$$

and， $\text{IoU}_\text{pred}^\text{truth}$ represents the intersection over union between the predicted box and the ground box(object).

Based on the definition of `confidence`, this `confidence` reflects:

1. $\text{Pr(object)}$: how confident the model is that this predicted box contains an object.

2. $\text{IoU}_\text{pred}^\text{truth}$: how accurate the model thinks that the predicted box is that it predicts.


- 注3. Conditional class probability, $ \text{Pr} \left( \text{Class}_i \vert \text{Object} \right) $. These probabilities are conditioned on **the grid cell containing an object**. Only predict one set of class probabilities per grid cell, regardless of the number of boxes B.

- 注4. At the *test time*, multipy the conditional class probabilities and the individual box confidence predictions,

$$
\text{Pr} \left(\text{Class}_i \vert \text{Object} \right) * \text{Pr(object)} * \text{IoU}_\text{pred}^\text{truth} = \text{Pr} \left(\text{Class}_i \right) * \text{IoU}_\text{pred}^\text{truth},
$$

which gives class-specific confidence scores for each box. These scores encode **the probability of that class appearing in the box** and **how well the predicted box fits the object**.

## $Loss$ 函数

### $Loss$ 函数

依据原文 $Loss$ 函数，基于我的理解，将 $Loss$ 函数写为

$$
\begin{aligned} 
Loss =& \quad \lambda_\text{coord} \sum_{i=1}^{S^2} \sum_{j=1}^{B} \mathbb{1}_{ij}^\text{obj}\left[(x_i - \hat{x}_{ij})^2 + (y_i - \hat{y}_{ij})^2 \right]   \qquad \qquad & (x\text{-}y \textit{ section}) \\
& + \lambda_\text{coord} \sum_{i=1}^{S^2} \sum_{j=1}^{B} \mathbb{1}_{ij}^\text{obj} \left[ \left({w_i}^{1/2} - {\hat{w}_{ij}}^{1/2} \right)^2 + \left({h_i}^{1/2} - {\hat{h}_{ij}}^{1/2} \right)^2 \right]  \qquad \qquad &(w\text{-}h \textit{ section})\\
& + \sum_{i=1}^{S^2} \sum_{j=1}^{B} \mathbb{1}_{ij}^\text{obj}\left(C_i - \hat{C}_{ij} \right)^2 \quad (\text{actually, this } C_i = 1 )  \qquad \qquad & (\textit{obj confidence section}) \\  
& + \lambda_\text{noobj} \sum_{i=1}^{S^2} \sum_{j=1}^{B} \mathbb{1}_{ij}^\text{noobj}\left(C_i - \hat{C}_{ij} \right)^2 \quad (\text{actually, this } C_i = 0 )  \qquad \qquad & (\textit{noobj confidence section})\\
& + \sum_{i=1}^{S^2} \mathbb{1}_{i}^\text{obj} \sum_{i\in \text{classes} } \left( p_i(c) - \hat{p_{i}}(c) \right)^2
\qquad \qquad & (\textit{class probs section}) \end{aligned}
$$

> 1. 较原文，有一类改动: 把 $i=0, j=0$ 全部分别改为 $i=1, j=1$， 我认为这样更符合数学表达，虽然c中索引起始是0 

> 2. 较原文， 有一类添加, 即添加下标 `$j$`： 将`$\hat{x}_i, \hat{y}_i, \hat{w}_i, \hat{h}_i, \hat{C}_i$` 全部分别改为`$\hat{x}_{ij}, \hat{y}_{ij}, \hat{w}_{ij}, \hat{h}_{ij}, \hat{C}_{ij}$` , 因为在我看来, 这样更符合表达，利于理解. 需要注意的是，用一个下标`$i$`, 也没有原则问题， 因为training时每个grid cell 中**至多**只有一个predicted box会进入到`$Loss$`函数中的(x,y)、(w,h)、obj confidence 三个section.


### `$Loss$` 函数几处设计考虑

1. set $\lambda_\text{coord}=5, \ \lambda_\text{noobj}=0.5$，这两点考虑

    一是donot weight localization error equally  with classification error. 
    
    二是in every image many grid cells do not contain any
object. This pushes the “confidence” scores of those cells
towards zero, **often overpowering the gradient from cells
that do contain objects**(**不理解**, 为什么？“). This can lead to model instability,
causing training to diverge (发散) early on.

    To remedy the above two points, we increase the loss from bounding box
coordinate predictions and decrease the loss from confi-
dence predictions for boxes that don’t contain objects. We
use two parameters, `$\lambda_\text{coord}$` and `$\lambda_\text{noobj}$`to accomplish this.


2. 采用 `$w^{1/2},h^{1/2}$`, 而不是一次项

    这是考虑到Our error metric should reflect that
small deviations in large boxes matter less than in small
boxes. To partially address this we predict the square root
of the bounding box width and height instead of the width
and height directly
    
    在靠近原点的地方，1/2次幂函数的梯度变化显然比一次项函数的更大. 从而，1/2次幂函数较一次函数可以更好地反映：小变化对小目标的影响更显著. 
    
### training's `$Loss$` 函数计算流程图

下面针对each grid cell（下面简称为“cell”）整理`$Loss$`函数计算流程.


```
graph TD
A[cell] --> B[B pred bboxes]
A --> C[C class probabilities]
B --> D{a gt box's center is in cell?}
C --> D
D --> E{No}
E --> F[计算noobj confidence loss *注1]
F --> K[返回 Loss]
D --> G{Yes}
G --> H[计算class probs loss]
G --> I{计算B个pred bboxes与当前gt box的IoU}
I --> |取最大IoU的pred box作为best box| J[只将best box代入, 计算x-y, w-h, obj confidence loss, 除best box之外的noobj confidence loss *注2,注3]
H --> L[返回loss]
J --> L
```
> 注1. 当 all of gt boxes' center is not in the cell `$i$` 时，
$$
\mathbb{1}_i^\text{obj} = 0, \quad
\mathbb{1}_{ij}^\text{obj}=0 (j = 1, ... ,B), \quad \mathbb{1}_{ij}^\text{noobj}=1 (j = 1, ... ,B).
$$

> 注2. 当 a gt box's center is in the cell `$i$` 时，
$$
\begin{aligned}
\mathbb{1}_i^\text{obj} = 1, \quad
\mathbb{1}_{ij}^\text{obj}=1 \ (j = \text{best\_box\_index}), \\ 
\quad \mathbb{1}_{ij}^\text{noobj}=1 \ (j = 1, ... ,B \ \text{and}\ j \not=  \text{best\_box\_index}).
\end{aligned}
$$

> 注3. 当grid cell含object时(gt box's center is in cell), 除去best box，Darknet源码detection_layer.c中还会计算剩余pred boxes的noobj confidence. 这应该是合理的，因为剩余pred boxes与gt boxes的IoU不是最大的，是应该惩罚的

> 注4. **疑问:** 对于不含object的predicted boxes的惩罚会不会轻了, 毕竟`$\lambda_\text{noobj}=0.5$`, 太小了? 但是，注意到，在一张图片，包含 object 的 predicted bbox 占少数，不含的是多数，这样来看，惩罚也是有效果的， 不会太小.

## 关于Darknet layer的解释 

1. `[route] layer` - is the same as Concat-layer in the Caffe
layers=-1, -4 means that will be concatenated two layers, with relative indexies -1 and -4

2. `[reorg] layer` - just reshapes feature map - decreases size and increases number of channels, without changing elements.
stride=2 mean that width and height will be decreased by 2 times, and number of channels will be increased by 2x2 = 4 times, so the total number of element will still the same:

$$
\begin{aligned}
\text{width\_old} * \text{height\_old} & *  \text{channels\_old} \\
&= \text{width\_new} * \text{height\_new} * \text{channels\_new}
\end{aligned}
$$
